{
  "company_name": "Broadcom",
  "quarter": "Q3",
  "fiscal_year": "2024",
  "speakers": {
    "hock_tan": {
      "role": "President, Chief Executive Officer, and Director",
      "responses": [
        {
          "topic": "AI Revenue Growth and Forecast",
          "content": "Well, as we indicated in the last earnings call, the VMware business continues to book very well as we convert our customers very much in two ways, one from perpetual to a subscription license but also those subscription licenses for the full stack of VCF. And that has been very successful, as I indicated, given the high ratio of VCF subscribers, new subscribers that we have achieved. And we see this trend continuing in Q4 very much so and very likely through into '25. So, in terms of directional trend, other than the guidance I'm giving you in Q4 '24, directionally, we continue to see accelerated bookings and, by extension, accelerated growth."
        },
        {
          "topic": "Competition with Nvidia in AI Accelerators and Ethernet Switching",
          "content": "So, a very interesting question, Vivek. On AI accelerators, I think we are operating on a different, to start with, scale, much as different model. It is -- you know, that -- the GPUs, which are the AI accelerator of choice on merchant -- in a merchant environment, is something that is extremely powerful as a model, and it's something that Nvidia operates in a very, very effective manner. We don't even think about competing against them in that space, not in the least.\n\nThat's where they're very good at and we know where we stand with respect to that. Now, what we do for very selected -- or selective hyperscalers is if they have the scale and the skills to try to create silicon solutions, which are AI accelerators, to do particular AI -- very complex AI workloads, we're happy to use our IP portfolio to create those custom ASIC AI accelerator. So, I do not see them as truly competing against each other. And far for me to say I'm trying to position myself to be a competitor on basically GPUs in this market.\n\nWe're not. We are not competitor to them. We don't try to be either. Now, on networking, maybe that's different.\n\nBut again, they may -- people may be approaching it and they may be approaching it from a different angle than we are. We are, as I indicated all along, very deep in Ethernet as we've been doing Ethernet for over 25 years, Ethernet networking, and we've gone through a lot of market transitions, and we have captured a lot of market transitions from cloud-scale networking to routing and, now, AI. So, it's a natural extension for us to go into AI. We also recognize that being the AI compute engine of choice in merchant's -- in the ecosystem, which is GPUs, that they are trying to create a platform that is probably end-to-end very integrated.\n\nWe take the approach that we don't do those GPUs, so -- but we enable the GPUs to work very well. So, if anything else, we supplement and hopefully complement those GPUs in -- with customers who are building bigger and bigger GPU clusters."
        },
        {
          "topic": "AI Revenue Guidance Conservative Estimate",
          "content": "Because I guided just over 8 -- over 11 billion, Stacy. It could be what you think it is. You know, it's -- quarterly shipments get sometimes very lumpy, and it depends on rate of deployment. It depends a lot of things.\n\nSo, you may be right. You may be -- you may get -- you may estimate it better than I do, but the general trend trajectory is it's getting better."
        },
        {
          "topic": "Networking Product Cadence and Competitive Positioning",
          "content": "Well, you know what, sometimes, you have to let things take its time. But it's two-year cadence, so we're right on. Late '23 was when we showed it out through our Tomahawk 5, and it adopted -- adoption. You're correct.\n\nWith AI, it has been tremendous because of the -- it ties in with the need for very large bandwidth in the networking -- in the fabric for AI clusters -- AI data centers. But regardless, we've always targeted Tomahawk 6 to be out two years after that, which should put it into late '25."
        },
        {
          "topic": "Mapping GPU Demand to AI Networking Opportunities",
          "content": "There is, but it's so complex, I stopped creating such a model, Tim. I'm serious. But there is because one would say that for -- yeah, for every $1 billion you spend on GPU, you probably would spend probably on networking. And if you include the optical interconnects as part of it, though we are not totally in that market, except for the components like DSPs, lasers, PIN diodes that go into those high bandwidth optical connects.\n\nBut if you just take optical connects in totality, switching, all the networking components that goes into -- attaches itself to clustering a bunch of GPUs, you probably would say that about 25% of the value of the GPU goes to networking, the rest of networking. Now, not entirely all of it is my available market. I don't do the optical connects, but I do the few components I talked about in it. But roughly, the simple way to look at it is probably about 25%, maybe 30% of all these infrastructure components is kind of attached to the GPU value point itself.\n\nBut having said that, it's never -- one, we're never that precise that deployment is the same way. So, you may see the deployment of GPUs or the purchase of GPU much earlier and the networking comes later or sometimes less, the other way around, which is why you're seeing the mix going on within my AI revenue mix. But typically, you run toward that range over time."
        },
        {
          "topic": "Custom ASIC AI Business Profitability and Competition",
          "content": "Let me take the second part first, which is our AI accelerate -- custom AI accelerator business. It is a very profitable business. And let me put the scale in -- look -- examine it from a model point of view. I mean, you know, each of these AI accelerators, no different from a GPU.\n\nThe way this -- we do -- these large language models get run computing, get run on these accelerators. No one single accelerator, as you know, can run these big large language models. You need multiple of it, no matter how powerful those accelerators are. But also -- and the way the models are run, there's a lot of memory -- access to memory requirements.\n\nSo, each of these accelerator comes with a large amount of cache memory, as you call it, what you guys probably now know as HBM, high bandwidth memory, specialized for AI accelerators of GPUs. So, we supply both in our custom business. And the logic side of it, the -- you know, where you -- where the compute function is on doing the chips, the margin there are no different than the margin in any -- in most of any of our semiconductor silicon chip business. But when you attach to it a huge amount of memory -- memory comes from a third party.\n\nThere are a few memory makers who make this specialized thing. We don't do margin stacking on that. So, buy -- almost buying basic math will dilute the margin of these AI accelerators when you sell them with memory, which we do. It does push up revenue somewhat higher, but it is -- dilute the margin.\n\nBut regardless, the spend, the R&D, the opex that goes to support this as a percent of the revenue, which is higher revenue, is so much less. So, on an operating margin level, this is easily as profitable, if not more profitable, given the scale that each of those custom AI accelerator can go up to. It's even better than our normal operating margin scale. So, that's the return on investment that attracts and keeps us going at this game.\n\nAnd this is more than a game. It's a very difficult business. And to answer your first question, there's only one Broadcom, period."
        },
        {
          "topic": "Software Business Run Rate and VMware Cloud Foundation Adoption",
          "content": "Well, on the second one, I don't know about any crowding out, to be honest. It's not -- what we're offering, obviously, is not something that they would like to use themselves -- to be able to do themselves, which is they are already spending on building their own on-prem data centers. And typical approach people take, a lot of enterprises take historically, continued today, than most people do, a lot of people do is they have best of breed. What I mean was they create a data center that is compute as a separate category, best compute there is, and they often enough use vSphere for compute virtualization due to improved productivity, but best of breed there.\n\nThen they have best of breed on networking and best of breed on storage with a common management operations layer, which sometimes -- very often is also VMware vRealize. And what we're trying to say is this mixed bag -- and what they see is this mixed bag best-of-breed data center, very heterogeneous, is not grieving that -- it's not a highly resilient data center. I mean, you have a mixed bag, so it goes down. Where do you find quickly root cause? Everybody is pointing fingers at the other.\n\nSo, you got a problem, not very resilient and not necessarily secure between bare metal in one side and software on the other side. So, it's a natural thinking on the part of many CIOs we talk to to say, hey, I want to create one common platform, as opposed to just best of breed of each. So, that gets us into that. So, if it's a greenfield, that's not bad.\n\nThey started from scratch. If it's a brownfield, that means they have existing data centers, trying to upgrade, it's -- that's -- sometimes that's more challenging for us to get that adopted. So, I'm not sure there's a crowding out here. There's some competition obviously on Greenfield where they can spend a budget on an entire platform versus best of breed.\n\nBut on the existing data center where you're trying to upgrade, that's a trickier thing to do, and it cuts the other way as well for us. But -- so, that's how I see it. So, in that sense, best answer is I don't think we are seeing a level of crowding out that is -- any and that's very significant for me to mention. In terms of the revenue mix, no, Brocade is having a great, great field year, so far, and still chugging along.\n\nBut will that sustain? Hell no. You know that. Brocade goes through cycles, like most enterprise purchases. So, we're enjoying it while it lasts."
        },
        {
          "topic": "Networking Switch Portfolio and Hyperscaler Preference",
          "content": "Well, let me have Charlie address this question. He's the expert.\n\nCharlie B. Kawwas -- President, Semiconductor Solutions\n\nYeah. Thank you, Hock. So, two quick things on this. One is the -- you're exactly right that the portfolio we have is quite unique in providing that flexibility.\n\nAnd by the way, this is exactly why Hock and his statements earlier on mentioned that seven out of the top eight hyperscalers use our portfolio, and they use it specifically because it provides that flexibility. So, whether you have an architecture that's based on an endpoint and you want to actually build your platform that way or you want that switching to happen in the fabric itself, that's why we have the full end-to-end portfolio. So, that, actually, has been a proven differentiator for us. And then on top of that, we've been working, as you know, to provide a complete network operating system that's open on top of that using SONiC and SAI, which has been deployed in many of the hyperscalers.\n\nAnd so, the combination of the portfolio, plus the stack, really differentiates the solution that we can offer to these hyperscalers. And if they decide to build their own NICs, their own accelerators are custom or use standard products, whether it's from Broadcom or other, that platform, that portfolio of infrastructure switching gives you that full flexibility."
        }
      ]
    },
    "kirsten_spears": {
      "role": "Chief Financial Officer and Chief Accounting Officer",
      "responses": [
        {
          "topic": "AI Revenue Mix and Q4 Outlook",
          "content": "Yeah. Well, as we indicated in the last earnings call, for this past quarter, I think we're talking about two-thirds in compute and one-third in networking. And we kind of expect Q4 to run the similar trend. And to answer your second part, no, we don't guide yet for fiscal '25, but we do expect fiscal '25 to continue to be strong, to show strong growth on our AI revenue."
        }
      ]
    },
    "charlie_kawwas": {
      "role": "President, Semiconductor Solutions",
      "responses": [
        {
          "topic": "Flexible Networking Switch Portfolio and Hyperscaler Preference",
          "content": "Yeah, thank you, Hock. So, two quick things on this. One is the -- you're exactly right that the portfolio we have is quite unique in providing that flexibility.\n\nAnd by the way, this is exactly why Hock and his statements earlier on mentioned that seven out of the top eight hyperscalers use our portfolio, and they use it specifically because it provides that flexibility. So, whether you have an architecture that's based on an endpoint and you want to actually build your platform that way or you want that switching to happen in the fabric itself, that's why we have the full end-to-end portfolio. So, that, actually, has been a proven differentiator for us. And then on top of that, we've been working, as you know, to provide a complete network operating system that's open on top of that using SONiC and SAI, which has been deployed in many of the hyperscalers.\n\nAnd so, the combination of the portfolio, plus the stack, really differentiates the solution that we can offer to these hyperscalers. And if they decide to build their own NICs, their own accelerators are custom or use standard products, whether it's from Broadcom or other, that platform, that portfolio of infrastructure switching gives you that full flexibility."
        }
      ]
    }
  },
  "metadata": {
    "parsed_date": "2024-09-15T09:00:00.000Z",
    "company_ticker": "AVGO",
    "source": "earnings_call_transcript"
  }
}
