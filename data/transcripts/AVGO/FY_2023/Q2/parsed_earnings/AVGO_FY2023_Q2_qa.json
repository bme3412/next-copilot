{
  "company_name": "Broadcom",
  "quarter": "Q2",
  "fiscal_year": "2023",
  "speakers": {
    "hock_tan": {
      "role": "President and Chief Executive Officer",
      "responses": [
        {
          "topic": "AI Demand Evolution and Competition",
          "content": "OK. Well, on your first part of your question, yes, we -- I mean, last earnings call, we have indicated there was a strong sense of demand, and we have seen that continue unabated in terms of that strong demand such that's coming in. Now, of course, we all realize lead times -- manufacturing lead times on most of these cutting-edge products is fairly extended. I mean, you don't make this -- manufacture these products under our process anything less than six months or thereabouts.\n\nAnd while there is strong demand and a strong urgency of demand, the ability to ramp up will be more measured and addressing demands that are most urgent. On the second part, no, we've always seen competition. And really, even in traditional workloads in enterprise data centers and hyperscale data centers, our business, our markets in networking, switching, routing continues to face competition. So really nothing new here.\n\nCompetition continues to exist, and we -- each of us do the best we can in the areas we are best at doing."
        },
        {
          "topic": "ASIC and Switch Revenue Targets",
          "content": "On your first part of your question -- you guys love your question in two parts, let's do the first part first. We guided or we indicated that for fiscal '23 that the revenue we are looking in this space is $3.8 billion. There's no reason nor are we trying to do it now in the middle of the year to change that forecast at this point. So, we still keep to that forecast we've given you in fiscal '23.\n\nWe're obviously giving you a sense of trajectory in my remarks on what we see '24 to look like. And that, again, is a broad trajectory of the guidance, nothing more than that, just to give you a sense for the accelerated move from '22, '23 and headed into '24. Nothing more than that. But in terms of specific numbers that you indicated we gave, it's -- we stay by our forecast of fiscal '23, 3.8%, frankly, because my view, it's a bit early to give you any revised forecast.\n\nThen beyond that, on your most broad specific question, ASICs versus merchant, I always favor merchant, whether it's in compute, whether it's in networking. In my mind, long-term, merchant will eventually, in my view, have a better shot at prevailing. But what we're not talking -- what we're talking about today is, obviously, a shorter term issue versus a very long-term issue. And the shorter term issue is, yes, compute offload exists.\n\nBut again, the number of players in compute offload ASICs is very, very limited, and that's what we continue to see."
        },
        {
          "topic": "Next-Generation AI and Compute Offload Programs",
          "content": "Well, it's -- we are -- our basic opportunity still lies in the networking of AI networks. And we have the products out there. And we are working with many, many customers, obviously, to put in place this disaggregated -- distributed, disaggregated architecture, which -- of Ethernet fabric on AI. And yes, that's a lot of obvious interest and lots of design that exists out there."
        },
        {
          "topic": "AI Spending Impact on Traditional Compute",
          "content": "Your guess is as good as mine, actually. I can tell you this. I mean, you're right, there's this AI networks and this budget that are now allocated more and more by the hyperscale toward this AI networks. But not necessarily, particularly in enterprise, at the expense of traditional workloads and traditional data centers.\n\nI think there's going to be -- there's definitely coexistence. And a lot of the large amount of spending on AI today that we see for us, that is very much on the hyperscale. And so, enterprises are still focusing a lot of their budgets as they have on the traditional data centers and traditional workloads supporting x86. But it's just maybe too early to -- really for us to figure out whether that is that cannibalization."
        },
        {
          "topic": "Lead Times and Manufacturing Cycle Times",
          "content": "By the way, it's 50. Yes, my standard lead time for our products is 50 weeks, and we are still staying with it because it's not about as much lead time to manufacture the products as our interest and, frankly, mutual interest between our customers and ourselves to take a hard look at providing visibility for us in ensuring we can supply and supply in the right amount at the right time the requirements. So yes, we're still sticking to 50 weeks."
        },
        {
          "topic": "AI Revenue Share and Cannibalization",
          "content": "Answer -- from an earlier question by a peer of yours, we do not see -- obviously, we do not know, we do not see cannibalization, but these are early innings, relatively speaking, and budgets don't change that rapidly. If there's cannibalization, obviously, it comes from where the spending goes in terms of priority. It's not obvious to us there is that clarity to be able to tell you there's cannibalization, not in the least. And by the way, if you look at the numbers that all the growth is coming from it, perhaps you're right.\n\nBut as we talk -- as we sit here in '23 and we still show some level of growth, I would say, we still show growth in the rest of our business, in the rest of products, augmented -- perhaps that growth is augmented with the growth in our AI revenue, in delivering AI products, but it's not entirely all our growth. I would say at least half the growth is still on our traditional business, the other half may be out of generative AI."
        },
        {
          "topic": "Electro-Optic Portfolio in AI Networking",
          "content": "Look, what you say is very, very insightful. It's -- a big part of our growth now in AI comes from the networking components that we're supplying into creating this Ethernet fabric for AI clusters. In fact, a big part of it, you hit on. And the rate of growth there is probably faster than our offload computing can grow.\n\nAnd that's where we are focused on, as I say, our networking products are merchant standard products, supporting the very rapid growth of generative AI clusters out there in the compute side. And for us, this growth in the networking side is really the faster part of the growth."
        },
        {
          "topic": "Wireless Contract Renewal and Content Visibility",
          "content": "OK. Well, our long-term collaboration agreement that we recently announced, it includes, as it indicated, wireless connectivity and 5G components. It does not include the high-performance analog components, mixed signal components that we also sell to the North American OEM customer. right? That doesn't make it any less, I would add, strategic, not deeply engaged with each other. I would definitely hasten to add.\n\nAnd on the second part, Ed, if you could indulge me, could you repeat that question?"
        },
        {
          "topic": "Semiconductor Business Long-Term Growth and R&D",
          "content": "Very, very good question, Toshiya. Well, we are still a very broadly diversified semiconductor company, as I pointed out, with multiple -- with still multiple end markets beyond just AI, most of which AI revenue happen to sit in my networking segment of the business, as you all noted, and you see. So we still have plenty of others. And even as I mentioned, for fiscal '24, our view is that it could hit over 25% of our semiconductor revenue.\n\nWe still have many large number of underpinnings for the rest of our semiconductor business. I mean, our wireless business, for instance, has a very strong lease of life for multi-years, and that's a big chunk of business. Just that the AI business appears to be trying to catch up to it in terms of the size. But our broadband server storage enterprise business continues to be very, very sustainable.\n\nAnd when you mix it all up, I don't know, we haven't updated our forecast long-term, so to show. I really have nothing more to add than what we already told you in the past. Would it have -- make a difference in our long-term growth rate? Don't know. We haven't thought about it.\n\nI'll leave it to you to probably speculate before I put anything on paper."
        },
        {
          "topic": "Foundry Relationships and Pricing Strategy",
          "content": "Thank you. We tend to be very loyal to our suppliers. The same reason we look at customers, the same -- in that same manner, it cuts both ways for us. So, there's a deep abiding loyalty in all our key suppliers.\n\nHaving said that, we also have to be very realistic of the geopolitical environment we have today. And so, we are also very open to looking at in certain specific technologies to broaden our supply base. And we have taken steps to constantly look at it, much as we still continue to want to be very loyal and fair to our existing base. So -- and so we continue that way.\n\nAnd because of that partnership and loyalty, for us, price increase is something that is a very long-term thing, it's part of the overall relationship. And put it simply, we don't move just because of prices. We stay put because of support, service and abiding sense of -- a very abiding sense of commitment mutually."
        },
        {
          "topic": "Wireless Deal Clarification and Electro-Optic Demand",
          "content": "You're not wrong. All this, as I indicated upfront in my remarks, current remarks, yes, we see our next generation coming up Tomahawk 5, which will have silicon photonics, which is co-packaging as a key part of that offering and not to mention that it's going up to 51 terabit per second cut-through bandwidth. It's exactly what you want to put in place for very high demanding AI networks, especially if those AI networks start running to -- over 32,000 GPU clusters running at 800 gigabit per second. Then you really need a big amount of switching because those kind of networks, as I mentioned, have to be very low latency, virtually lossless.\n\nEthernet lossless calls for some interesting science and technology in order to make Ethernet lossless. Because by definition, Ethernet tends to have it traditionally. But the technology is there to make it lossless. So all this fits in with our new generation of products.\n\nAnd not to mention our Jericho3-AI, which, as you know, the router has a unique differentiated technology that allows for very, very low tail latency and in terms of how it transmits and reorder packets so that there's no loss and very little latency. And that exists in network routing in telcos, which we now apply to AI networks in a very effective manner, and that's our whole new generation products. So yes, we're leaning into this opportunity with our networking technology and next-generation products very much. So, you hit it right on, and which is, one, makes it very exciting for us in AI.\n\nIt's in the networking area, networking space that we see most interesting opportunities."
        },
        {
          "topic": "Compute Offload Business Growth and Customer Concentration",
          "content": "Thank you. Good question. And I'll reiterate the answers in some other ways I've given to certain other audience who have asked this question. We really have only one rail customer -- one customer.\n\nAnd in my forecast, in my remarks so far in offload computing, it's pretty much very, very largely around one customer. It's not very diversified. It's very focused. That's our compute offload business."
        },
        {
          "topic": "Software Gross Margins and Brocade Stabilization",
          "content": "OK. Well, our software segment comprises, you hit it correctly, two parts. That's our core software products revenues and sold directly to enterprises. And these are your typical infrastructure software products.\n\nAnd they are multiyear contracts. And we have ton -- and we have a lot of backlog, something like $17 billion of backlog, averaging over almost two and a half, three years. And every quarter, a part of that renews, and we give you the data on it. It's very stable.\n\nAnd given our historical pattern of renewing on expanding consumption of our core group of customers, we tend to drive that in a very stable manner. And the growth rate is very, very predictable, and we're happy with that. Then we overlay on it a business that is software, but also very appliance different, the fiber channel SAN business of Brocade. And that's very enterprise-driven, very, very much so.\n\nOnly used by enterprises, obviously, and large enterprises at that. And it is a fairly cyclical business. And last year was a very strong up cycle. And this year, not surprisingly, the cycles are not as strong, especially compared year on year to the very strong numbers last year.\n\nSo, that's -- well, this is the phenomenon -- the outcome of the combining the two is what we're seeing today. But given another -- my view next year, the cycle could turn around and Brocade would go on. And then, instead of a 3% year-on-year growth in this whole segment, we could end up with high single digits year-on-year growth rate because the core software revenue, as I've always indicated to you guys, you want to plan long term on mid-single-digit year-on-year growth rate. And that's very predictable part of our numbers."
        },
        {
          "topic": "Content Uplift for AI Servers vs General Compute",
          "content": "I'm sorry to disappoint you on your two parts, but it's too early for me to be able to give you a good answer or a very definitive answer on that. Because by far the majority of servers today are your traditional servers driving x86 CPUs. And the networking today are very, very still running Ethernet traditional data center networking. Because most enterprises if not virtually, all enterprises today are very much still running their own traditional servers on x86.\n\nGenerative AI is something so new and in a way, so -- the limits of it is so extended that what we largely see today are at the hyperscale guys in terms of deploying at scale those generative AI infrastructures. Enterprises continue to deploy and operate standard x86 servers and Ethernet networking in the traditional data centers. And so, that's still -- so what we're seeing today may be early part of the whole cycle, that's your question, which is why I cannot give you any definitive view, opinion of how -- what the attach rate, what the ratio will be or if there's any stability that could be achieved anywhere in the near term. We both -- we see both running and coexisting very much together."
        }
      ]
    },
    "kirsten_spears": {
      "role": "Chief Financial Officer",
      "responses": []
    }
  },
  "analyst_questions": [
    {
      "analyst": "Ross Seymore",
      "firm": "Deutsche Bank",
      "topics": [
        "AI Demand Evolution",
        "Competitive Implications"
      ],
      "questions": "Thanks for letting me ask a question. Hock, I just will start off with the topic that you started, AI these days is everywhere. Thanks for the color that you gave in the percentage of the sales that it was potentially going to represent into the future. I wanted to just get a little bit more color on two aspects of that.\n\nHow you've seen the demand evolve during the course of your quarter? Has it accelerated, in what areas, etc.? And is there any competitive implications for it? We've heard from some of the compute folks that they want to do more on the networking side. And then, obviously, you want to do more into the compute side. So I just wondered how the competitive intensity is changing, given the AI workload increases these days."
    },
    {
      "analyst": "Vivek Arya",
      "firm": "Bank of America Merrill Lynch",
      "topics": [
        "ASIC Revenue Targets",
        "Switch Revenue Targets",
        "ASIC vs General-Purpose GPU"
      ],
      "questions": "Thanks for taking my questions. Hock, I just wanted to first clarify. I think you might have mentioned it, but I think last quarter, you gave very specific numerical targets of $3 billion in ASICs and $800 million in switches for fiscal '23. I just wanted to make sure if there is any specific update to those numbers.\n\nIs it more than $4 billion in total now, etc.? And then, my question is, longer term, what do you think the share is going to be between kind of general purpose GPU-type solutions versus ASICs? Do you think that share shifts toward ASICs? Do you think it shifts toward general purpose solutions? Because if I look outside of the compute offload opportunity, you have generally favored, right, more the general purpose market. So, I'm curious, how do you see this share between general purpose versus ASICs play out in this AI processing opportunity longer term?"
    },
    {
      "analyst": "Harlan Sur",
      "firm": "J.P. Morgan",
      "topics": [
        "Next-Generation AI and Compute Offload Programs",
        "Packaging Constraints"
      ],
      "questions": "Hi. Good afternoon. Thanks for taking my question. Great to see the strong and growing ramp of your AI compute offload and networking products.\n\nOn your next generation -- Hock, on your next-generation AI and compute offload programs that are in the design phase now, you've got your next-gen switching and routing platforms that are being qualified. Like, are your customers continuing to push the team to accelerate the design funnel, pull in program ramp timing? And then, I think you might have addressed this, but I just wanted to clarify, all of these solutions use the same type of very advanced packaging, like stack die, HBM memory -- packaging. And not surprisingly, this is the same architecture used by your AI GPU peers, which are driving the same strong trends, right? So is the Broadcom team facing or expected to face like advanced packaging, advanced substrate supply constraints? And how is the operations team going to sort of manage through all of this?"
    },
    {
      "analyst": "Timothy Arcuri",
      "firm": "UBS",
      "topics": [
        "AI Spending Impact",
        "Traditional Compute Infrastructure"
      ],
      "questions": "Thanks a lot. Hock, I was wondering if you can sort of help shed some light on the general perception that all this AI spending is sort of boxing out traditional compute. Can you talk about that? Or is it that just capex budgets are going to have to grow to support all this extra AI capex? I mean, the trick is probably somewhere in between, but I'm wondering if you can help shed some light on just the general perception that all of this is coming at the expense of the traditional compute and the traditional infrastructure. Thanks."
    },
    {
      "analyst": "Ambrish Srivastava",
      "firm": "BMO Capital Markets",
      "topics": [
        "Lead Times",
        "Manufacturing Cycle Times"
      ],
      "questions": "Hi. Thank you very much. Hock, I have a less sexy topic to talk about, but obviously very important in how you manage the business. Can you talk about lead times and especially in the light of demand moderating, manufacturing cycle times coming down, not to mention the six months that you highlighted for the cutting edge? Are you still staying with the 52-week kind of lead quoting to customers, or has that changed? Thank you."
    },
    {
      "analyst": "Harsh Kumar",
      "firm": "Piper Sandler",
      "topics": [
        "AI Revenue Share",
        "Business Cannibalization"
      ],
      "questions": "Yes. Hey, Hock, I was hoping you could clarify something for us. I think earlier in the beginning of the call when you gave your AI commentary, you said that gen AI revenues are 15% odd today, they'll go to 25% by the end of 2024. That's practically all your growth.\n\nThat's the $4 billion -- $3 billion, $4 billion that you'll grow. So looking at your commentary, I know your core business is doing really well. So I know that I'm probably misinterpreting it. But I was hoping that maybe there's not so many -- hoping that there's no cannibalization going on in your business, but maybe you could clarify for us."
    },
    {
      "analyst": "Karl Ackerman",
      "firm": "Exane BNP Paribas",
      "topics": [
        "Electro-Optic Portfolio",
        "AI Networking"
      ],
      "questions": "Thank you for taking my question. Hock, you rightly pointed to the custom silicon opportunity that supports your cloud AI initiatives. However, your AI revenue that's not tied to custom silicon appears to be doubling in fiscal '23. And the outlook for fiscal '24 implies that it will double again.\n\nObviously, Broadcom has multiple areas of exposure to AI really across PCI switches, Tomahawk, Jericho and Ramon ASICs and electro-optics. I guess what sort of opportunity do you see your electric optics portfolio playing a role in high-performance networking environments for inferencing and training AI applications?"
    },
    {
      "analyst": "Joseph Moore",
      "firm": "Morgan Stanley",
      "topics": [
        "Wireless Contract Renewal",
        "Content Visibility"
      ],
      "questions": "Great. Thank you. I wanted to ask about the renewal of the wireless contract. Can you give us a sense for how much sort of concrete visibility you have into content over the duration of that? As you mentioned, it's both, RF and wireless connectivity.\n\nJust any the additional color you can give us would be great."
    },
    {
      "analyst": "Toshiya Hari",
      "firm": "Goldman Sachs",
      "topics": [
        "Semiconductor Business Long-Term Growth",
        "AI-Driven Growth",
        "R&D Investment"
      ],
      "questions": "Hi. Thank you so much for taking the question. Hock, I'm curious how you're thinking about your semiconductor business long term. You've discussed AI pretty extensively throughout this call.\n\nCould this be something that drives higher growth for your semiconductor business on a sustained basis? I think historically, you've given relatively subdued or muted growth rates for your business vis-Ã -vis many of your competitors. Is this something that can drive sustained growth acceleration for your business? And if so, how should we think about the rate of R&D growth going forward as well? Because I think your peers are growing R&D faster than what you guys are doing today. Thank you."
    },
    {
      "analyst": "William Stein",
      "firm": "Truist Securities",
      "topics": [
        "Foundry Relationships",
        "Pricing Strategy"
      ],
      "questions": "Great. Thank you. Hock, I'm wondering if you can talk about your foundry relationships. You've got a very strong relationship with TSM.\n\nAnd of course, Intel has been very vocal about winning new customers potentially. I wonder if you can talk about your flexibility and openness and considering new partners. And then, maybe also talk about pricing from foundry and whether that's influencing any changes quarter-to-quarter. There have been certainly a lot of price increases that we've heard about recently, and I'd love to hear your comments.\n\nThank you."
    },
    {
      "analyst": "Edward Snyder",
      "firm": "Charter Equity Research",
      "topics": [
        "Wireless Deal Clarification",
        "Electro-Optic Demand"
      ],
      "questions": "Thank you very much. Hock, basically housekeeping question. It sounded like your comments in the press release on the wireless deal did not include Mixed Signal, which is part of your past agreement. And everything you've seen to have said today doesn't -- suggest that may not be the next -- in wireless and RF, but you're also doing a lot of mixed single stuff, too.\n\nSo maybe you can provide some clarity on that. And now also, why shouldn't we expect the increased interest in AI to increase the prospects, if not orders immediately for the electro-optic products that are coming on discipline? So I would think that would be much greater demand, given the clusters and the size of these arrays that people are trying to put together, provide enormous benefits, I think, in power. Maybe give us some color on that."
    },
    {
      "analyst": "Antoine Chkaiban",
      "firm": "New Street Research",
      "topics": [
        "Compute Offload Growth",
        "Customer Concentration"
      ],
      "questions": "Hi. Thank you very much for the question. I'll stick to a single-part question. Can you maybe double-click on your computes offload business? What can you maybe tell us about how growth could split between revenues from existing customers or potential diversification of that business going forward? Thank you."
    },
    {
      "analyst": "Kurt Swartz",
      "firm": "Evercore ISI",
      "topics": [
        "Software Gross Margins",
        "Brocade Stabilization"
      ],
      "questions": "Hey. Thank you. This is Kurt Swartz on for C.J. I wanted to touch on software gross margins, which continue to tick higher alongside softness in Brocade.\n\nCurious what sort of visibility you may have into Brocade stabilization and how we should think about software gross margins as mix normalizes. Thank you."
    },
    {
      "analyst": "Vijay Rakesh",
      "firm": "Mizuho Securities",
      "topics": [
        "Content Uplift for AI Servers",
        "Generative AI Server Uptake"
      ],
      "questions": "Yes. Hi, Hock, just a quick -- I'll keep it a two-part question for you to wrap up. So just wondering what the content uplift for Broadcom is on an AI server versus a general compute server. And if you look at generative AI, what percent of servers today are being outfitted for generative AI as you look? You have a dominant share there.\n\nAnd where do you see that uptake ratio for generative AI and year out if you look at fiscal '24, '25?"
    }
  ],
  "metadata": {
    "parsed_date": "2023-06-15T10:00:00.000Z",
    "company_ticker": "AVGO",
    "source": "earnings_call_transcript"
  }
}
