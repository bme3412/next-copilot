{
  "company_name": "Oracle",
  "quarter": "Q4",
  "fiscal_year": "2024",
  "speakers": {
    "ken_bond": {
      "role": "Head of Investor Relations",
      "responses": [
        {
          "topic": "Closing Remarks",
          "content": "Thank you, John. A telephonic replay of this conference call would be available for 24 hours on our Investor Relations website. Thank you for joining us today. And with that, I'll turn the call back to Krista for closing."
        }
      ]
    },
    "safra_ada_catz": {
      "role": "Chief Executive Officer",
      "responses": [
        {
          "topic": "OCI Growth and Capacity",
          "content": "It's all about capacity. It is -- as we bring the capacity online, wherever it's going online around the world, is when those workloads are coming over. A lot of the engineering work was -- is done in advance so that those customers know how they can operate. They bring smaller workloads.\n\nBut the bigger workloads, they -- they're just waiting for us to go online and make it available to them. It is really that level. We are scheduling them on our availability. And as I mentioned, our pipeline, to take more deals, is all about us just getting the capacity up and live and moving forward."
        },
        {
          "topic": "Expense Synergy and Margins",
          "content": "It's all about capacity. It is -- as we bring the capacity online, wherever it's going online around the world, is when those workloads are coming over. A lot of the engineering work was -- is done in advance so that those customers know how they can operate. They bring smaller workloads.\n\nBut the bigger workloads, they -- they're just waiting for us to go online and make it available to them. It is really that level. We are scheduling them on our availability. And as I mentioned, our pipeline, to take more deals, is all about us just getting the capacity up and live and moving forward."
        }
      ]
    },
    "larry_j_ellison": {
      "role": "Chairman and Chief Technology Officer",
      "responses": [
        {
          "topic": "OCI Trajectory and AI Backlog",
          "content": "Yeah. Well, again, in OCI, we've talked for a while about our ability to build very small data centers, one you could put in a ship or a submarine, or a full cloud, a full Oracle Cloud we will soon have in six standard half racks to go into a conventional data center. So, virtually, any one of our customers could choose to have the full Oracle Cloud in their data center, with every service, every service in the cloud. And they could scale that up quite extraordinarily large.\n\nSo, we talked about the fact that we can start very small, and that's a huge difference between us and our competitors. So, we can actually put it again customer by customer, small countries, we can do. What we haven't talked so much about is we're also building the largest data centers in the world. We talked about -- I think we talked briefly about one last call where we can park -- it's a 70-megawatt data center where we can park eight 747s, nose to tail, in the data center.\n\nIt's a huge AI training data center. Well, we're also building a 200-megawatt data center. In fact, we -- this past quarter, we sold about half of that data center for the -- you know, for a period of time. So, we're now bringing 200-megawatt data centers online.\n\nSo, we are literally building the smallest, most portable, most affordable cloud data centers, all the way up to 200-megawatt data centers, ideal for training very large language models and keeping them up to date. This AI race is going to go on for a long time. It's not a matter of getting ahead -- just simply getting ahead in AI, but you also have to keep your model current, and that's going to take larger and larger data centers. And some of the data centers we have that we're planning are actually even bigger.\n\nThere -- some are getting very close to, dare I say it, a gigawatt, which is a pretty good-sized city or one enormous AI cloud training data center. No one else can span this range. And in every case, we have unbelievably fast networks that are a part of this. The data centers we're building include the power plants and the transmission of the power directly into the data center and liquid cooling and -- because these new -- these modern data centers are moving from air-cooled to liquid-cooled and you have to engineer them from scratch.\n\nAnd that's what we've been doing for some time, and that's what we'll continue to do. And currently, we are leading the pack in being able to deliver that quality and that scale of data center."
        },
        {
          "topic": "OCI Architecture and Competitive Advantage",
          "content": "Well, I think, in OCI, we've talked for a while about our ability to build very small data centers, one you could put in a ship or a submarine, or a full cloud, a full Oracle Cloud we will soon have in six standard half racks to go into a conventional data center. So, virtually, any one of our customers could choose to have the full Oracle Cloud in their data center, with every service, every service in the cloud. And they could scale that up quite extraordinarily large.\n\nSo, we talked about the fact that we can start very small, and that's a huge difference between us and our competitors. So, we can actually put it again customer by customer, small countries, we can do. What we haven't talked so much about is we're also building the largest data centers in the world. We talked about -- I think we talked briefly about one last call where we can park -- it's a 70-megawatt data center where we can park eight 747s, nose to tail, in the data center.\n\nIt's a huge AI training data center. Well, we're also building a 200-megawatt data center. In fact, we -- this past quarter, we sold about half of that data center for the -- you know, for a period of time. So, we're now bringing 200-megawatt data centers online.\n\nSo, we are literally building the smallest, most portable, most affordable cloud data centers, all the way up to 200-megawatt data centers, ideal for training very large language models and keeping them up to date. This AI race is going to go on for a long time. It's not a matter of getting ahead -- just simply getting ahead in AI, but you also have to keep your model current, and that's going to take larger and larger data centers. And some of the data centers we have that we're planning are actually even bigger.\n\nThere -- some are getting very close to, dare I say it, a gigawatt, which is a pretty good-sized city or one enormous AI cloud training data center. No one else can span this range. And in every case, we have unbelievably fast networks that are a part of this. The data centers we're building include the power plants and the transmission of the power directly into the data center and liquid cooling and -- because these new -- these modern data centers are moving from air-cooled to liquid-cooled and you have to engineer them from scratch.\n\nAnd that's what we've been doing for some time, and that's what we'll continue to do. And currently, we are leading the pack in being able to deliver that quality and that scale of data center."
        },
        {
          "topic": "OCI Data Center Automation",
          "content": "Well, you know, our -- you know, we have -- we're on our second generation of data center and -- and our second generation of cloud. Now, a lot of people noticed that we were a little bit late to the party, but that's because we moved from a generation which we were not very happy with to a second generation, which we think solved a lot of problems that the other cloud companies have not yet solved. So, the non-blocking ultra-fast RDMA network is not only useful for AI -- training AI models; it's useful for almost everything. It's certainly useful for building a much faster database.\n\nIt's useful for -- in terms of the automation level we have in our data centers. Our data centers are 100% automated. They can figure themselves; they run themselves. We don't have a lot of labor now.\n\nThat -- that saves us a huge amount of money. A lot of labor costs is saved, but the biggest advantage is if you don't have human beings involved, you don't have human labor, you don't have human errors, you don't have mistakes, you can ensure security. Most security problems are caused by people that make mistakes or people that engage in mischief. We don't have that in our data center.\n\nThat's another huge advantage. Our data centers are -- because they're all -- they're identical, the only way we could automate them was to make them all the same. And they vary only by scale. There are big ones and small ones, but they're identical.\n\nThey all have the same hardware pieces and the same software pieces. They all have the same automation, and that automation allows us to put these data centers in very small countries. We expect to have many many more data centers than any other cloud provider, but we also put those data centers --- customers. Nomura Research, NRI, which resells Oracle Cloud capacity in Japan, has two dedicated regions and are building two more.\n\nThey run the Tokyo Stock Exchange. I don't know of any clouds that are running stock exchanges other than ours. And again, it's because of the extreme reliability and security that we get with all of the automation that's included, you know, with our data center. So, we have cost advantages; we have performance advantages; we have security advantages.\n\nAnd that's why we're growing much faster than any of the other hyperscalers."
        },
        {
          "topic": "Generative AI and Autonomous Database",
          "content": "Well, if you're -- you're constantly training these models. Keep in mind, you have to bring in new data if you're in -- obviously, in the healthcare field, in the legal field. New -- new cases are being judged. New research is being published all the time.\n\nAnd for your AI models to be relevant, they have to be up to date. So, it's not that you train and then do nothing but inferencing thereafter. So, your training and your inferencing sit right next to each other. As long as we can do this stuff twice as fast as everybody else, that's on the -- by the way, not just on the training side, that's also on the inferencing side, then we're going to be half the cost or better.\n\nSo, we think we're going to be very very competitive across the board, whether it's training or in inferencing. So, we don't -- so we're pretty confident that we've got a cost-performance advantage. Again, if you run twice as fast in the cloud, you cost half as much because you pay by the hour. So, the performance advantage is really an enormous cost advantage for us.\n\nWe don't see that going away anytime soon. And it applies to inferencing as well as -- as well as training. Now, as far as GPUs, is that our GPUs are a low-margin business, not for -- 100% automated cloud with very very low costs. We think, in some cases, our prices for GPU training, which are very profitable, by the way, for us but are often lower -- you know, our prices are lower than the costs of other hyperscalers doing the training.\n\nI think we're going to be very very competitive across the board, whether it's training or in inferencing. So, we don't -- so we're pretty confident that we've got a cost-performance advantage."
        }
      ]
    },
    "analyst_questions": [
      {
        "analyst": "Mark Moerdler",
        "firm": "Bernstein",
        "topics": [
          "OCI Architecture",
          "Data Center Regions",
          "Capex Growth"
        ],
        "questions": [
          {
            "question_text": "Thank you very much for taking my question, and congratulations on the quarter. I'd like to gain better insight into OCI. OCI is designed differently from its larger peers, deliverable in just 10 and shortly three racks of servers. Can you explain how the architectural difference impacts how you build and deliver OCI racks and how it will impact how many data center regions you could have compared to your peers? And then, how might this affect your capex growth over the next five years? Thanks."
          }
        ]
      },
      {
        "analyst": "Siti Panigrahi",
        "firm": "Mizuho Securities",
        "topics": [
          "OCI Growth",
          "Database Migration",
          "Multi-Cloud Strategy",
          "Customer Preparedness",
          "Revenue Contribution"
        ],
        "questions": [
          {
            "question_text": "Thanks for taking my question. It's impressive to see OCI growth 52% and even database as a service up 28%. So, as you execute on your multi-cloud strategy, can you provide some color on the database migration to cloud or even database as a service traction? And what are you hearing from customers as they prepare to migrate on-prem database to cloud? And how should we think about the contribution to revenue going forward?"
          }
        ]
      },
      {
        "analyst": "Brad Zelnick",
        "firm": "Deutsche Bank",
        "topics": [
          "AI Infrastructure",
          "GPU Clusters",
          "Scaling Laws",
          "Capex and Technology Leadership"
        ],
        "questions": [
          {
            "question_text": "Great. Thank you, and congrats on the continued acceleration at scale. Larry, as Oracle leads with larger and larger GPU clusters, there's healthy industry debate around scaling laws, with Elon Musk allegedly pushing the envelope of what's possible. I'm curious to hear your perspective as to whether there's diminishing returns on the amount of compute thrown at model training in Oracle's ongoing leadership in AI infrastructure and GPU super clusters.\n\nThanks."
          }
        ]
      },
      {
        "analyst": "Raimo Lenschow",
        "firm": "Barclays",
        "topics": [
          "SaaS Growth",
          "Back Office Systems",
          "AI Integration",
          "Booking Trends"
        ],
        "questions": [
          {
            "question_text": "Perfect. Thank you. Can I shift gear a little bit and go back to the SaaS part of the business? If you look there, if you look at your growth rates, I mean, we're at kind of a late-stage point of the cycle where people usually don't look at their back office systems. But you guys are putting up like really solid growth numbers there.\n\nYou're outgrowing your peers. Can you talk a little bit of what's going on there? Thank you."
          }
        ]
      },
      {
        "analyst": "Keith Weiss",
        "firm": "Morgan Stanley",
        "topics": [
          "Cerner Modernization",
          "Expense Synergy",
          "Margin Profile",
          "Cerner Expectations"
        ],
        "questions": [
          {
            "question_text": "Excellent. Thank you, guys, and thank you for taking the question. I wanted to drill in on Cerner, basically the one-year anniversary of that acquisition, and maybe from Larry get an update on where we are with modernizing that solution and modernizing that product; and basically, whether Cerner has kind of lived up to your expectations thus far. And then, maybe for Safra, if we could dig into the expense synergy side of the equation.\n\nYou guys have done a great job increasing margins on a year-on-year basis in this quarter. How much is left to go within Cerner and getting that margin profile to match the broader Oracle margin profile?"
          }
        ]
      }
    ],
    "metadata": {
      "parsed_date": "2024-12-15T00:00:00Z",
      "company_ticker": "ORCL",
      "source": "Q4 2024 Earnings Call Transcript",
      "operating_environment_challenges": [
        "Macroeconomic uncertainty and potential recession impacts",
        "High competition in cloud infrastructure and application markets",
        "Integration of acquired companies (e.g., Cerner)",
        "Customer demand for flexible, cost-effective, and high-performance cloud solutions",
        "Technological advancements and competition in generative AI and machine learning workloads",
        "Maintaining and expanding multi-cloud partnerships",
        "Supply chain constraints affecting cloud and hardware delivery"
      ],
      "executive_changes": {
        "new_leadership": {
          "name": "N/A",
          "role": "N/A",
          "start_date": "N/A",
          "context": "N/A"
        },
        "leadership_transition": {
          "name": "N/A",
          "role": "N/A",
          "end_date": "N/A",
          "successor": {
            "name": "N/A",
            "role": "N/A",
            "context": "N/A"
          }
        }
      }
    }
  }
}
